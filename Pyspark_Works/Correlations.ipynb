{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init(\"/usr/local/spark\")\n",
    "import pyspark\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "import collections \n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "from pyspark.sql.functions import rand, randn\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setMaster(\"local\")\n",
    "sc.stop()\n",
    "sc=SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColStat Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean for column 1 is 2.0\n",
      "Variance for column 1 is 1.0\n",
      "Number of non-zeros for column 1 is 3.0\n",
      "\n",
      "Mean for column 2 is 20.0\n",
      "Variance for column 2 is 100.0\n",
      "Number of non-zeros for column 2 is 3.0\n",
      "\n",
      "Mean for column 3 is 200.0\n",
      "Variance for column 3 is 10000.0\n",
      "Number of non-zeros for column 3 is 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#df = sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "mat = sc.parallelize(\n",
    "    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])])\n",
    "\n",
    "#mat = sc.parallelize(\n",
    "#    [sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)), sqlContext.range(0, 10).withColumn('rand2', randn(seed=10))]\n",
    "#)  # an RDD of Vectors\n",
    "\n",
    "# Compute column summary statistics.\n",
    "summary=Statistics.colStats(mat)\n",
    "for i in range(3):\n",
    "    print(\"\\n\"+ \"Mean for column %(n)s is %(s)s\" % {'n': int(i+1), 's': summary.mean()[i]} )\n",
    "    print(\"Variance for column %(n)s is %(s)s\" % {'n': int(i+1), 's': summary.variance()[i]} )\n",
    "    print(\"Number of non-zeros for column %(n)s is %(s)s\" % {'n': int(i+1), 's': summary.numNonzeros()[i]} )# a dense vector containing the mean value for each column\n",
    "    #print(summary.variance()[i])  # column-wise variance\n",
    "    #print(summary.numNonzeros()[i])  # number of nonzeros in each column\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o95.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/burak/train_sample2.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7bef254ad92f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/burak/train_sample2.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrdd_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrdd_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \"\"\"\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \"\"\"\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \"\"\"\n\u001b[1;32m--> 361\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \"\"\"\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m   1326\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2516\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2517\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2519\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o95.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/burak/train_sample2.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.textFile('/home/burak/train_sample2.txt')\n",
    "rdd_split = rdd.map(lambda x: x.split(\"\\t\"))\n",
    "df=rdd_split.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(df.columns)):\n",
    "    df = df.withColumn(\"_\"+str(i+1), df[\"_\"+str(i+1)].cast(IntegerType()))\n",
    "    \n",
    "df_new =df.select(\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\", \"_7\", \"_8\", \"_9\", \"_10\", \"_11\", \"_12\", \"_13\", \"_14\")\n",
    "df_new=df_new.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "mat1=df_new.rdd.map(lambda row : Vectors.dense([item for item in row]))\n",
    "summary=Statistics.colStats(mat1)\n",
    "for i in range(len(df_new.columns)):\n",
    "    print(\"\\n\"+ \"Mean for column %(n)s is %(s)s\" % {'n': int(i+1), 's': summary.mean()[i]} )\n",
    "    print(\"Variance for column %(n)s is %(s)s\" % {'n': int(i+1), 's': summary.variance()[i]} )\n",
    "    print(\"Number of non-zeros for column %(n)s is %(s)s\" % {'n': int(i+1), 's': summary.numNonzeros()[i]} )# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.textFile('/home/burak/train_sample2.txt')\n",
    "rdd1_split = rdd1.map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd1_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-791a7c7dabf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrdd1_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rdd1_split' is not defined"
     ]
    }
   ],
   "source": [
    "df=rdd1_split.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.columns)):\n",
    "    df = df.withColumn(\"_\"+str(i+1), df[\"_\"+str(i+1)].cast(IntegerType()))\n",
    "    \n",
    "df_new =df.select(\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\", \"_7\", \"_8\", \"_9\", \"_10\", \"_11\", \"_12\", \"_13\", \"_14\")\n",
    "df_new=df_new.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=[x for x in df_new.columns], outputCol=\"features\")\n",
    "\n",
    "assembled = assembler.transform(df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assembled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e59f0b0ed051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0massembled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'assembled' is not defined"
     ]
    }
   ],
   "source": [
    "assembled.select(\"features\").rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it seen we transformed our data set to vectorized dense and sparse vector data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[  1.00000000e+00,   1.05793760e-01,   3.43892970e-02,\n",
      "               -1.73036823e-03,  -5.47143906e-02,  -7.61936253e-02,\n",
      "               -6.13991220e-02,   8.85211203e-02,  -1.98359199e-02,\n",
      "                1.99188471e-02,   1.97233541e-01,   1.72041424e-01,\n",
      "                3.28722890e-02,  -3.49612313e-02],\n",
      "             [  1.05793760e-01,   1.00000000e+00,   2.90884508e-02,\n",
      "                5.31573539e-03,   6.19514288e-02,  -6.39554475e-02,\n",
      "               -5.51209479e-02,   4.68818352e-01,   1.66518948e-02,\n",
      "                4.85451193e-02,   4.27889568e-01,   2.49332850e-01,\n",
      "                8.38467677e-02,  -5.22343654e-03],\n",
      "             [  3.43892970e-02,   2.90884508e-02,   1.00000000e+00,\n",
      "               -9.10402700e-03,  -8.34786817e-02,  -8.08840712e-03,\n",
      "               -1.05089887e-02,   1.40080961e-02,  -1.28803943e-02,\n",
      "               -7.32862226e-03,   2.20151767e-02,   1.64339847e-02,\n",
      "               -1.27215090e-03,  -1.34249389e-02],\n",
      "             [ -1.73036823e-03,   5.31573539e-03,  -9.10402700e-03,\n",
      "                1.00000000e+00,   4.63020869e-02,  -5.46277365e-03,\n",
      "                6.24017932e-03,   1.59493966e-03,   5.85276064e-02,\n",
      "                3.33263915e-02,  -5.46829915e-03,   8.28766118e-04,\n",
      "               -1.70401537e-03,   6.19249230e-02],\n",
      "             [ -5.47143906e-02,   6.19514288e-02,  -8.34786817e-02,\n",
      "                4.63020869e-02,   1.00000000e+00,  -8.40015313e-02,\n",
      "                2.14967551e-02,   3.09780250e-02,   4.14048969e-01,\n",
      "                2.56644443e-01,   1.33994285e-01,   5.28250927e-02,\n",
      "                1.47820053e-02,   4.34105094e-01],\n",
      "             [ -7.61936253e-02,  -6.39554475e-02,  -8.08840712e-03,\n",
      "               -5.46277365e-03,  -8.40015313e-02,   1.00000000e+00,\n",
      "                1.12248173e-02,  -5.22695271e-02,  -3.19635078e-02,\n",
      "               -5.35125208e-02,  -1.51497918e-01,  -1.10099363e-01,\n",
      "               -1.69056939e-02,  -1.97076994e-02],\n",
      "             [ -6.13991220e-02,  -5.51209479e-02,  -1.05089887e-02,\n",
      "                6.24017932e-03,   2.14967551e-02,   1.12248173e-02,\n",
      "                1.00000000e+00,  -2.57615079e-02,   1.13616769e-03,\n",
      "                1.90956974e-01,  -1.27464506e-01,  -3.73369642e-02,\n",
      "               -9.42266891e-03,   1.68240912e-02],\n",
      "             [  8.85211203e-02,   4.68818352e-01,   1.40080961e-02,\n",
      "                1.59493966e-03,   3.09780250e-02,  -5.22695271e-02,\n",
      "               -2.57615079e-02,   1.00000000e+00,   1.44987925e-02,\n",
      "                1.97682086e-01,   2.36890367e-01,   6.53314576e-01,\n",
      "                8.66383030e-02,  -3.34980423e-03],\n",
      "             [ -1.98359199e-02,   1.66518948e-02,  -1.28803943e-02,\n",
      "                5.85276064e-02,   4.14048969e-01,  -3.19635078e-02,\n",
      "                1.13616769e-03,   1.44987925e-02,   1.00000000e+00,\n",
      "                2.13121307e-01,   2.97381861e-02,   2.56981066e-02,\n",
      "                3.98564999e-03,   8.41031285e-01],\n",
      "             [  1.99188471e-02,   4.85451193e-02,  -7.32862226e-03,\n",
      "                3.33263915e-02,   2.56644443e-01,  -5.35125208e-02,\n",
      "                1.90956974e-01,   1.97682086e-01,   2.13121307e-01,\n",
      "                1.00000000e+00,   6.51113881e-02,   3.66570290e-01,\n",
      "                3.53647756e-02,   2.41286634e-01],\n",
      "             [  1.97233541e-01,   4.27889568e-01,   2.20151767e-02,\n",
      "               -5.46829915e-03,   1.33994285e-01,  -1.51497918e-01,\n",
      "               -1.27464506e-01,   2.36890367e-01,   2.97381861e-02,\n",
      "                6.51113881e-02,   1.00000000e+00,   3.79167543e-01,\n",
      "                6.54329879e-02,  -5.41213899e-03],\n",
      "             [  1.72041424e-01,   2.49332850e-01,   1.64339847e-02,\n",
      "                8.28766118e-04,   5.28250927e-02,  -1.10099363e-01,\n",
      "               -3.73369642e-02,   6.53314576e-01,   2.56981066e-02,\n",
      "                3.66570290e-01,   3.79167543e-01,   1.00000000e+00,\n",
      "                8.77210365e-02,  -5.23675747e-03],\n",
      "             [  3.28722890e-02,   8.38467677e-02,  -1.27215090e-03,\n",
      "               -1.70401537e-03,   1.47820053e-02,  -1.69056939e-02,\n",
      "               -9.42266891e-03,   8.66383030e-02,   3.98564999e-03,\n",
      "                3.53647756e-02,   6.54329879e-02,   8.77210365e-02,\n",
      "                1.00000000e+00,  -5.30159985e-04],\n",
      "             [ -3.49612313e-02,  -5.22343654e-03,  -1.34249389e-02,\n",
      "                6.19249230e-02,   4.34105094e-01,  -1.97076994e-02,\n",
      "                1.68240912e-02,  -3.34980423e-03,   8.41031285e-01,\n",
      "                2.41286634e-01,  -5.41213899e-03,  -5.23675747e-03,\n",
      "               -5.30159985e-04,   1.00000000e+00]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(assembled, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation matrix:\n",
      "DenseMatrix([[ 1.        ,  0.21129819,  0.00859781, -0.10776546, -0.06546042,\n",
      "              -0.16112493, -0.06627577,  0.22903443, -0.05978631,  0.00888983,\n",
      "               0.19577371,  0.2295929 ,  0.05599314, -0.15922038],\n",
      "             [ 0.21129819,  1.        ,  0.02889873,  0.08348082,  0.19385097,\n",
      "              -0.61799393, -0.12016754,  0.48141287,  0.21028064,  0.04108718,\n",
      "               0.93457238,  0.4379942 ,  0.17088045,  0.0511587 ],\n",
      "             [ 0.00859781,  0.02889873,  1.        ,  0.04002237,  0.09939959,\n",
      "              -0.02212945,  0.01410925,  0.00855099,  0.06417939,  0.03303829,\n",
      "               0.02731779,  0.00949485,  0.00447007,  0.11444111],\n",
      "             [-0.10776546,  0.08348082,  0.04002237,  1.        ,  0.52350619,\n",
      "              -0.07434861,  0.09258774,  0.02823411,  0.38311021,  0.11718579,\n",
      "               0.07529945,  0.02217507,  0.01518783,  0.52416839],\n",
      "             [-0.06546042,  0.19385097,  0.09939959,  0.52350619,  1.        ,\n",
      "              -0.17339692,  0.18396527,  0.16921666,  0.61683535,  0.32443092,\n",
      "               0.1859131 ,  0.16263087,  0.05939976,  0.85027466],\n",
      "             [-0.16112493, -0.61799393, -0.02212945, -0.07434861, -0.17339692,\n",
      "               1.        ,  0.23451974, -0.27923183, -0.14655278,  0.05298143,\n",
      "              -0.62102913, -0.29299105, -0.12451523, -0.02563338],\n",
      "             [-0.06627577, -0.12016754,  0.01410925,  0.09258774,  0.18396527,\n",
      "               0.23451974,  1.        ,  0.38156062,  0.24793445,  0.55005252,\n",
      "              -0.12996321,  0.38802901,  0.02818549,  0.27864592],\n",
      "             [ 0.22903443,  0.48141287,  0.00855099,  0.02823411,  0.16921666,\n",
      "              -0.27923183,  0.38156062,  1.        ,  0.24180118,  0.47752898,\n",
      "               0.41900045,  0.89859429,  0.21313835,  0.08513749],\n",
      "             [-0.05978631,  0.21028064,  0.06417939,  0.38311021,  0.61683535,\n",
      "              -0.14655278,  0.24793445,  0.24180118,  1.        ,  0.47013822,\n",
      "               0.19224531,  0.25537949,  0.06880569,  0.61440375],\n",
      "             [ 0.00888983,  0.04108718,  0.03303829,  0.11718579,  0.32443092,\n",
      "               0.05298143,  0.55005252,  0.47752898,  0.47013822,  1.        ,\n",
      "               0.03028122,  0.5242128 ,  0.08837053,  0.33007197],\n",
      "             [ 0.19577371,  0.93457238,  0.02731779,  0.07529945,  0.1859131 ,\n",
      "              -0.62102913, -0.12996321,  0.41900045,  0.19224531,  0.03028122,\n",
      "               1.        ,  0.42807203,  0.15216727,  0.04779471],\n",
      "             [ 0.2295929 ,  0.4379942 ,  0.00949485,  0.02217507,  0.16263087,\n",
      "              -0.29299105,  0.38802901,  0.89859429,  0.25537949,  0.5242128 ,\n",
      "               0.42807203,  1.        ,  0.19588844,  0.0904532 ],\n",
      "             [ 0.05599314,  0.17088045,  0.00447007,  0.01518783,  0.05939976,\n",
      "              -0.12451523,  0.02818549,  0.21313835,  0.06880569,  0.08837053,\n",
      "               0.15216727,  0.19588844,  1.        ,  0.03415249],\n",
      "             [-0.15922038,  0.0511587 ,  0.11444111,  0.52416839,  0.85027466,\n",
      "              -0.02563338,  0.27864592,  0.08513749,  0.61440375,  0.33007197,\n",
      "               0.04779471,  0.0904532 ,  0.03415249,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r2 = Correlation.corr(assembled, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation means to a relationship or association between quantities. It is useful to express onw\n",
    "quantity in terms of its relationship with others.\n",
    "\n",
    "    1- Pearson Correlation Coefficient\n",
    "Pearson Correlation Coefficient measures the statistical relationship or linear association\n",
    "between two continuous variables. Pearson Correlation Coefficient gives information about\n",
    "the magnitude or correlation of the relationship as well as the direction of the relationship.\n",
    "Because it is based on the covariance method, it is known one of the best methods of\n",
    "measuring the relationship between the variables of interest. In other words, this coefficient\n",
    "determines the degree to which the relationship between the two variables can be defined\n",
    "by a line. This collation technique can be used if at least one of the variables is distributed\n",
    "near normal or normal.\n",
    "\n",
    "    2- Spearman’s Rank Correlation\n",
    "Spearman’s Rank Correlation can be used to measure the degree of association between two\n",
    "variables. It can be defined as a specific Pearson ρ example applied to the ordinal variables.\n",
    "Unlike Pearson, Spearman&#39;s correlation is not limited to linear relations. Instead, it measures\n",
    "the monotonic association between two variables and is relies on the rank order of the\n",
    "values. In other words, instead of comparing averages and variances, the Spearman\n",
    "coefficient looks in the order of relative value for each variable. This makes it suitable for use\n",
    "with continuous and discrete data. It can be used when both variables are not normally\n",
    "distributed, and also it can be used when at least one of the variables is an ordinal variable."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
