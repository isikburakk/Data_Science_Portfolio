{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, VectorSlicer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setMaster(\"local\")\n",
    "sc=SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Temerrut_flag: integer (nullable = true)\n",
      " |-- Basvuru_tarihi: string (nullable = true)\n",
      " |-- Kullanim_orani: double (nullable = true)\n",
      " |-- Musteri_yasi: integer (nullable = true)\n",
      " |-- Dpd_30_adeti: integer (nullable = true)\n",
      " |-- Borcun_gelire_orani: double (nullable = true)\n",
      " |-- Gelir: double (nullable = true)\n",
      " |-- Acik_kredi_sayisi: integer (nullable = true)\n",
      " |-- Onceki_temerrut_sayisi: integer (nullable = true)\n",
      " |-- Acik_ev_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_2_yil_dpd_31_60_adeti: integer (nullable = true)\n",
      " |-- Bakmakla_yukumlu_kisi_sayisi: double (nullable = true)\n",
      " |-- Guncel_yil_ort_vdsz_mev_tutari: double (nullable = true)\n",
      " |-- Onceki_yil_max_vdsz_mev_tutari: double (nullable = true)\n",
      " |-- Onceki_yil_min_vdsz_mev_tutari: double (nullable = true)\n",
      " |-- Rotatif_krediler_kullanim_orani: string (nullable = true)\n",
      " |-- Kredi_vadesi: string (nullable = true)\n",
      " |-- Guncel_yil_gunluk_ort_kk_risk_tutari: double (nullable = true)\n",
      " |-- Guncel_vdsz_mev_tutari: double (nullable = true)\n",
      " |-- SGK_kodu: double (nullable = true)\n",
      " |-- Evin_sahiplik_durumu: string (nullable = true)\n",
      " |-- Istenen_evin_fiyati: integer (nullable = true)\n",
      " |-- Toplam_calisma_suresi: double (nullable = true)\n",
      " |-- Onceki_yil_ort_vdsz_mev_tutari: double (nullable = true)\n",
      " |-- Calisma_tipi: integer (nullable = true)\n",
      " |-- Son_1_yil_kredi_basvuru_sayisi: integer (nullable = true)\n",
      " |-- Son_6_ay_kredi_basvuru_sayisi: integer (nullable = true)\n",
      " |-- Son_3_ay_kredi_basvuru_sayisi: integer (nullable = true)\n",
      " |-- Son_2_yil_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_1_yil_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_6_ay_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_3_ay_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- KK_flag: integer (nullable = true)\n",
      " |-- KK_limiti: double (nullable = true)\n",
      " |-- Dissal_kredi_skoru: integer (nullable = true)\n",
      " |-- Isyeri_tel_no: integer (nullable = true)\n",
      " |-- Evin_yasi: double (nullable = true)\n",
      " |-- Evin_bulundugu_sok_ort_ev_yasi: double (nullable = true)\n",
      " |-- En_son_calisma_gun_sayisi: double (nullable = true)\n",
      " |-- Yasadigi_yerin_nufus_yogunlugu: double (nullable = true)\n",
      " |-- Arac_yasi: double (nullable = true)\n",
      " |-- Arac_flag: integer (nullable = true)\n",
      " |-- Son_12_ay_kk_kullanim_orani: double (nullable = true)\n",
      " |-- Son_6_ay_kk_kullanim_orani: double (nullable = true)\n",
      " |-- Son_3_ay_kk_kullanim_orani: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data.csv')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['_c0', 'ID', 'Basvuru_tarihi']\n",
    "data = data.select([column for column in data.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, lit, sum\n",
    "\n",
    "def count_not_null(c, nan_as_null=False):\n",
    "    \"\"\"Use conversion between boolean and integer\n",
    "    - False -> 0\n",
    "    - True ->  1\n",
    "    \"\"\"\n",
    "    pred = col(c).isNotNull() & (~isnan(c) if nan_as_null else lit(True))\n",
    "    return sum(pred.cast(\"integer\")).alias(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = [(count_not_null(c) / count(\"*\")).alias(c) for c in data.columns]\n",
    "completeness = data.agg(*exprs).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Temerrut_flag': {0: 1.0},\n",
       " 'Kullanim_orani': {0: 1.0},\n",
       " 'Musteri_yasi': {0: 1.0},\n",
       " 'Dpd_30_adeti': {0: 1.0},\n",
       " 'Borcun_gelire_orani': {0: 1.0},\n",
       " 'Gelir': {0: 1.0},\n",
       " 'Acik_kredi_sayisi': {0: 1.0},\n",
       " 'Onceki_temerrut_sayisi': {0: 1.0},\n",
       " 'Acik_ev_kredi_sayisi': {0: 1.0},\n",
       " 'Son_2_yil_dpd_31_60_adeti': {0: 1.0},\n",
       " 'Bakmakla_yukumlu_kisi_sayisi': {0: 0.97384},\n",
       " 'Guncel_yil_ort_vdsz_mev_tutari': {0: 0.7908733333333333},\n",
       " 'Onceki_yil_max_vdsz_mev_tutari': {0: 0.7909466666666667},\n",
       " 'Onceki_yil_min_vdsz_mev_tutari': {0: 0.7909733333333333},\n",
       " 'Rotatif_krediler_kullanim_orani': {0: 0.7896733333333333},\n",
       " 'Kredi_vadesi': {0: 1.0},\n",
       " 'Guncel_yil_gunluk_ort_kk_risk_tutari': {0: 0.36102},\n",
       " 'Guncel_vdsz_mev_tutari': {0: 0.028373333333333334},\n",
       " 'SGK_kodu': {0: 0.01058},\n",
       " 'Evin_sahiplik_durumu': {0: 1.0},\n",
       " 'Istenen_evin_fiyati': {0: 1.0},\n",
       " 'Toplam_calisma_suresi': {0: 0.2904},\n",
       " 'Onceki_yil_ort_vdsz_mev_tutari': {0: 0.028373333333333334},\n",
       " 'Calisma_tipi': {0: 1.0},\n",
       " 'Son_1_yil_kredi_basvuru_sayisi': {0: 1.0},\n",
       " 'Son_6_ay_kredi_basvuru_sayisi': {0: 1.0},\n",
       " 'Son_3_ay_kredi_basvuru_sayisi': {0: 1.0},\n",
       " 'Son_2_yil_kabul_edilmis_kredi_sayisi': {0: 1.0},\n",
       " 'Son_1_yil_kabul_edilmis_kredi_sayisi': {0: 1.0},\n",
       " 'Son_6_ay_kabul_edilmis_kredi_sayisi': {0: 1.0},\n",
       " 'Son_3_ay_kabul_edilmis_kredi_sayisi': {0: 1.0},\n",
       " 'KK_flag': {0: 1.0},\n",
       " 'KK_limiti': {0: 0.5017},\n",
       " 'Dissal_kredi_skoru': {0: 1.0},\n",
       " 'Isyeri_tel_no': {0: 1.0},\n",
       " 'Evin_yasi': {0: 1.0},\n",
       " 'Evin_bulundugu_sok_ort_ev_yasi': {0: 1.0},\n",
       " 'En_son_calisma_gun_sayisi': {0: 0.018966666666666666},\n",
       " 'Yasadigi_yerin_nufus_yogunlugu': {0: 1.0},\n",
       " 'Arac_yasi': {0: 1.0},\n",
       " 'Arac_flag': {0: 1.0},\n",
       " 'Son_12_ay_kk_kullanim_orani': {0: 1.0},\n",
       " 'Son_6_ay_kk_kullanim_orani': {0: 1.0},\n",
       " 'Son_3_ay_kk_kullanim_orani': {0: 1.0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temerrut_flag: integer (nullable = true)\n",
      " |-- Kullanim_orani: double (nullable = true)\n",
      " |-- Musteri_yasi: integer (nullable = true)\n",
      " |-- Dpd_30_adeti: integer (nullable = true)\n",
      " |-- Borcun_gelire_orani: double (nullable = true)\n",
      " |-- Gelir: double (nullable = true)\n",
      " |-- Acik_kredi_sayisi: integer (nullable = true)\n",
      " |-- Onceki_temerrut_sayisi: integer (nullable = true)\n",
      " |-- Acik_ev_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_2_yil_dpd_31_60_adeti: integer (nullable = true)\n",
      " |-- Kredi_vadesi: string (nullable = true)\n",
      " |-- Evin_sahiplik_durumu: string (nullable = true)\n",
      " |-- Istenen_evin_fiyati: integer (nullable = true)\n",
      " |-- Calisma_tipi: integer (nullable = true)\n",
      " |-- Son_1_yil_kredi_basvuru_sayisi: integer (nullable = true)\n",
      " |-- Son_6_ay_kredi_basvuru_sayisi: integer (nullable = true)\n",
      " |-- Son_3_ay_kredi_basvuru_sayisi: integer (nullable = true)\n",
      " |-- Son_2_yil_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_1_yil_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_6_ay_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- Son_3_ay_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n",
      " |-- KK_flag: integer (nullable = true)\n",
      " |-- Dissal_kredi_skoru: integer (nullable = true)\n",
      " |-- Evin_yasi: double (nullable = true)\n",
      " |-- Evin_bulundugu_sok_ort_ev_yasi: double (nullable = true)\n",
      " |-- Yasadigi_yerin_nufus_yogunlugu: double (nullable = true)\n",
      " |-- Arac_yasi: double (nullable = true)\n",
      " |-- Arac_flag: integer (nullable = true)\n",
      " |-- Son_12_ay_kk_kullanim_orani: double (nullable = true)\n",
      " |-- Son_6_ay_kk_kullanim_orani: double (nullable = true)\n",
      " |-- Son_3_ay_kk_kullanim_orani: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['Bakmakla_yukumlu_kisi_sayisi', 'Guncel_yil_ort_vdsz_mev_tutari', 'Onceki_yil_max_vdsz_mev_tutari',\n",
    "            'Onceki_yil_min_vdsz_mev_tutari', 'Rotatif_krediler_kullanim_orani', 'Guncel_yil_gunluk_ort_kk_risk_tutari',\n",
    "            'Guncel_vdsz_mev_tutari', 'SGK_kodu', 'Toplam_calisma_suresi', 'Onceki_yil_ort_vdsz_mev_tutari', 'KK_limiti',\n",
    "            'En_son_calisma_gun_sayisi', 'Isyeri_tel_no']\n",
    "data = data.select([column for column in data.columns if column not in drop_list])\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 8:</b> DENEME\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Temerrut_flag: integer (nullable = true)\n",
      " |-- Musteri_yasi: integer (nullable = true)\n",
      " |-- Dpd_30_adeti: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deneme=[\"Temerrut_flag\",\"Musteri_yasi\",\"Dpd_30_adeti\"]\n",
    "data = data.select([column for column in data.columns if column in deneme])\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 8:</b> DENEME\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "# one hot encoding and assembling\n",
    "#encoding_var = [i[0] for i in df.dtypes if (i[1]=='string') & (i[0]!='y')]\n",
    "num_var = [i[0] for i in data.dtypes if ((i[1]=='int') | (i[1]=='double')) & (i[0]!='Temerrut_flag')]\n",
    "\n",
    "#string_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep') for c in encoding_var]\n",
    "#onehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c]) for c in encoding_var]\n",
    "#label_indexes = StringIndexer(inputCol = 'y', outputCol = 'label', handleInvalid = 'keep')\n",
    "assembler = VectorAssembler(inputCols = num_var , outputCol = \"features\")\n",
    "assembled = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([45.0, 2.0])),\n",
       " Row(features=DenseVector([40.0, 0.0])),\n",
       " Row(features=DenseVector([38.0, 1.0])),\n",
       " Row(features=DenseVector([30.0, 0.0])),\n",
       " Row(features=DenseVector([49.0, 1.0])),\n",
       " Row(features=DenseVector([74.0, 0.0])),\n",
       " Row(features=DenseVector([57.0, 0.0])),\n",
       " Row(features=DenseVector([39.0, 0.0])),\n",
       " Row(features=DenseVector([27.0, 0.0])),\n",
       " Row(features=DenseVector([57.0, 0.0]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembled.select(\"features\").rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled = assembled.orderBy(rand())\n",
    "# Split Data into Train / Test Sets\n",
    "train_data, test_data = assembled.randomSplit([.8, .2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0])),\n",
       " Row(features=DenseVector([21.0, 0.0]))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.select(\"features\").rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Classes\n",
    "nb_classes = train_data.select(\"Temerrut_flag\").distinct().count()\n",
    "\n",
    "# Number of Inputs or Input Dimensions\n",
    "input_dim = len(train_data.select(\"features\").first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Set up Deep Learning Model / Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(input_dim,), activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(256, activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               768       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 67,074\n",
      "Trainable params: 67,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The data format channels_last is not supported. Please use keras.backend.set_image_data_format(\"channels_first\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-212f7d14ea26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msystemml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeras2DML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msysml_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeras2DML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weights_dir'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# sysml_model.setConfigProperty(\"sysml.native.blas\", \"auto\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sysml_model.setGPU(True).setForceGPU(True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msysml_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\systemml\\mllearn\\estimators.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sparkSession, keras_model, input_shape, transferUsingDF, load_keras_weights, weights, labels, batch_size, max_iter, test_iter, test_interval, display, lr_policy, weight_decay, regularization_type)\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m             raise Exception('The data format ' + str(keras.backend.image_data_format())\n\u001b[1;32m-> 1039\u001b[1;33m                             + ' is not supported. Please use keras.backend.set_image_data_format(\"channels_first\")')\n\u001b[0m\u001b[0;32m   1040\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# Convert the sequential model to functional model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: The data format channels_last is not supported. Please use keras.backend.set_image_data_format(\"channels_first\")"
     ]
    }
   ],
   "source": [
    "from systemml.mllearn import Keras2DML\n",
    "sysml_model = Keras2DML(spark, model, weights='weights_dir')\n",
    "# sysml_model.setConfigProperty(\"sysml.native.blas\", \"auto\")\n",
    "# sysml_model.setGPU(True).setForceGPU(True)\n",
    "sysml_model.fit(X_train, y_train)\n",
    "sysml_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-55ceeede3e54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training time:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',#'sgd'\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy']) \n",
    "             \n",
    "             \n",
    "# Train your model, adjust batch size and epochs iteratively. Optionally time your training.\n",
    " \n",
    "import time\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=100) \n",
    "end=time.time()\n",
    "print(\"training time:\", (end-start))\n",
    "\n",
    "# Test your model on the secluded test set\n",
    "\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# Make predictions and Reshape your npndarrays to be able to verify your predictions by plotting out the image\n",
    "\n",
    "predictions = model.predict([X_test])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
