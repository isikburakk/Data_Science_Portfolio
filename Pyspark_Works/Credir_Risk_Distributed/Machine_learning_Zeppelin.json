{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562495817634_1936075306","id":"20190707-103657_928963678","dateCreated":"2019-07-07T10:36:57+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:255","text":"%spark.pyspark\n\nimport pyspark\nimport os\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nimport pyspark.sql.functions as F\nimport re\nimport collections \nimport multiprocessing \nfrom collections import Counter\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import isnan, when, count, col\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.sql.types import *\nimport sys \nfrom pyspark.ml.feature import Imputer\nfrom pyspark.sql.functions import length\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\nfrom pyspark.ml.feature import Imputer\nfrom pyspark.sql.functions import avg\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\nimport pyspark\nimport os\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nimport pyspark.sql.functions as F\nfrom pyspark.ml.feature import Imputer\nimport pandas as pd","dateUpdated":"2019-07-07T10:51:33+0000","dateFinished":"2019-07-07T10:51:33+0000","dateStarted":"2019-07-07T10:51:33+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 32: import pandas as pd\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1603510110579582163.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 32, in <module>\nImportError: No module named pandas\n"}]}},{"text":"%spark.pyspark\n\n\nBUCKET_NAME = 'powerpuffs'\nACCESS_KEY = 'AKIAJLZ6NR75UFVB673Q'\nSECRET_KEY = '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'","user":"anonymous","dateUpdated":"2019-07-07T10:45:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562495874668_-1894371122","id":"20190707-103754_1459737237","dateCreated":"2019-07-07T10:37:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:338","dateFinished":"2019-07-07T10:45:46+0000","dateStarted":"2019-07-07T10:45:46+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\nk\n\ndef print_spark_config(spark_context):\n    config_list = spark_context._conf.getAll()\n    for each in config_list:\n        print(each[0], each[1])\n\n\ndef spark_init(appname):\n\n    conf = SparkConf().setMaster(\"local[*]\") \\\n                        .setAppName(appname) \\\n                        .set(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n                        .set(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\")\n\n   \n    sc = SparkSession.builder.config(conf=conf).getOrCreate().sparkContext\n    sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n    hadoopConf = sc._jsc.hadoopConfiguration()\n    hadoopConf.set(\"fs.s3a.awsAccessKeyId\", ACCESS_KEY)\n    hadoopConf.set(\"fs.s3a.awsSecretAccessKey\", SECRET_KEY)\n    hadoopConf.set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\n    hadoopConf.set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\n    hadoopConf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")","user":"anonymous","dateUpdated":"2019-07-07T10:38:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562495891316_-720877085","id":"20190707-103811_1582915755","dateCreated":"2019-07-07T10:38:11+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:417","dateFinished":"2019-07-07T10:38:37+0000","dateStarted":"2019-07-07T10:38:37+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 1: k\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1603510110579582163.py\", line 375, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'k' is not defined\n"}]}},{"text":"%spark.pyspark\nsc = spark_init(\"puffs\")","user":"anonymous","dateUpdated":"2019-07-07T10:38:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562495917931_-1953247694","id":"20190707-103837_2114674699","dateCreated":"2019-07-07T10:38:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:489","dateFinished":"2019-07-07T10:38:56+0000","dateStarted":"2019-07-07T10:38:56+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\n\ns3_file_list = \"s3://\" + BUCKET_NAME + \"/\" + \"creadit_rist\"+ \"/\" +\"data0.csv\"\n\n\n\ns3_file_list","user":"anonymous","dateUpdated":"2019-07-07T10:45:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562495936249_-551014354","id":"20190707-103856_2033204440","dateCreated":"2019-07-07T10:38:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:577","dateFinished":"2019-07-07T10:45:56+0000","dateStarted":"2019-07-07T10:45:56+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"'s3://powerpuffs/creadit_rist/data0.csv'\n"}]}},{"text":"%spark.pyspark\ndata = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(\"s3://powerpuffs/creadit_rist/data0.csv\")\ndata.printSchema()","user":"anonymous","dateUpdated":"2019-07-07T10:45:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562495952515_1625355287","id":"20190707-103912_592405657","dateCreated":"2019-07-07T10:39:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:649","dateFinished":"2019-07-07T10:46:13+0000","dateStarted":"2019-07-07T10:45:58+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: integer (nullable = true)\n |-- ID: integer (nullable = true)\n |-- Temerrut_flag: integer (nullable = true)\n |-- Basvuru_tarihi: string (nullable = true)\n |-- Kullanim_orani: double (nullable = true)\n |-- Musteri_yasi: integer (nullable = true)\n |-- Dpd_30_adeti: integer (nullable = true)\n |-- Borcun_gelire_orani: double (nullable = true)\n |-- Gelir: double (nullable = true)\n |-- Acik_kredi_sayisi: integer (nullable = true)\n |-- Onceki_temerrut_sayisi: integer (nullable = true)\n |-- Acik_ev_kredi_sayisi: integer (nullable = true)\n |-- Son_2_yil_dpd_31_60_adeti: integer (nullable = true)\n |-- Bakmakla_yukumlu_kisi_sayisi: double (nullable = true)\n |-- Guncel_yil_ort_vdsz_mev_tutari: double (nullable = true)\n |-- Onceki_yil_max_vdsz_mev_tutari: double (nullable = true)\n |-- Onceki_yil_min_vdsz_mev_tutari: double (nullable = true)\n |-- Rotatif_krediler_kullanim_orani: string (nullable = true)\n |-- Kredi_vadesi: string (nullable = true)\n |-- Guncel_yil_gunluk_ort_kk_risk_tutari: double (nullable = true)\n |-- Guncel_vdsz_mev_tutari: double (nullable = true)\n |-- SGK_kodu: double (nullable = true)\n |-- Evin_sahiplik_durumu: string (nullable = true)\n |-- Istenen_evin_fiyati: integer (nullable = true)\n |-- Toplam_calisma_suresi: double (nullable = true)\n |-- Onceki_yil_ort_vdsz_mev_tutari: double (nullable = true)\n |-- Calisma_tipi: integer (nullable = true)\n |-- Son_1_yil_kredi_basvuru_sayisi: integer (nullable = true)\n |-- Son_6_ay_kredi_basvuru_sayisi: integer (nullable = true)\n |-- Son_3_ay_kredi_basvuru_sayisi: integer (nullable = true)\n |-- Son_2_yil_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n |-- Son_1_yil_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n |-- Son_6_ay_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n |-- Son_3_ay_kabul_edilmis_kredi_sayisi: integer (nullable = true)\n |-- KK_flag: integer (nullable = true)\n |-- KK_limiti: double (nullable = true)\n |-- Dissal_kredi_skoru: integer (nullable = true)\n |-- Isyeri_tel_no: integer (nullable = true)\n |-- Evin_yasi: double (nullable = true)\n |-- Evin_bulundugu_sok_ort_ev_yasi: double (nullable = true)\n |-- En_son_calisma_gun_sayisi: double (nullable = true)\n |-- Yasadigi_yerin_nufus_yogunlugu: double (nullable = true)\n |-- Arac_yasi: double (nullable = true)\n |-- Arac_flag: integer (nullable = true)\n |-- Son_12_ay_kk_kullanim_orani: double (nullable = true)\n |-- Son_6_ay_kk_kullanim_orani: double (nullable = true)\n |-- Son_3_ay_kk_kullanim_orani: double (nullable = true)\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-35-35.eu-central-1.compute.internal:4040/jobs/job?id=4","http://ip-172-31-35-35.eu-central-1.compute.internal:4040/jobs/job?id=5"],"interpreterSettingId":"spark"}}},{"text":"%spark.pyspark\ndrop_list = ['_c0', 'ID', 'Basvuru_tarihi']\ndata = data.select([column for column in data.columns if column not in drop_list])","user":"anonymous","dateUpdated":"2019-07-07T10:46:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562495979766_1507639295","id":"20190707-103939_557609738","dateCreated":"2019-07-07T10:39:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:737","dateFinished":"2019-07-07T10:46:38+0000","dateStarted":"2019-07-07T10:46:38+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n\nfrom pyspark.sql.functions import col, count, isnan, lit, sum\n\ndef count_not_null(c, nan_as_null=False):\n    \"\"\"Use conversion between boolean and integer\n    - False -> 0\n    - True ->  1\n    \"\"\"\n    pred = col(c).isNotNull() & (~isnan(c) if nan_as_null else lit(True))\n    return sum(pred.cast(\"integer\")).alias(c)","user":"anonymous","dateUpdated":"2019-07-07T10:46:49+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562496019995_-2059065528","id":"20190707-104019_2042512468","dateCreated":"2019-07-07T10:40:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:834","dateFinished":"2019-07-07T10:46:49+0000","dateStarted":"2019-07-07T10:46:49+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\nexprs = [(count_not_null(c) / count(\"*\")).alias(c) for c in data.columns]\ncompleteness = data.agg(*exprs).toPandas()","user":"anonymous","dateUpdated":"2019-07-07T10:47:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562496038833_-882264174","id":"20190707-104038_1052319969","dateCreated":"2019-07-07T10:40:38+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:915","dateFinished":"2019-07-07T10:47:02+0000","dateStarted":"2019-07-07T10:47:00+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 2: completeness = data.agg(*exprs).toPandas()\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1603510110579582163.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2075, in toPandas\n    require_minimum_pandas_version()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 129, in require_minimum_pandas_version\n    \"it was not found.\" % minimum_pandas_version)\nImportError: Pandas >= 0.19.2 must be installed; however, it was not found.\n"}]}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2019-07-07T10:47:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562496420231_1632968480","id":"20190707-104700_640035407","dateCreated":"2019-07-07T10:47:00+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1266"}],"name":"Machine_learning_Powerpuffs","id":"2EGVUR748","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}