{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I use PySpark, Keras, and Elephas python libraries to build an end-to-end deep learning pipeline that runs on Spark. Spark is an open-source distributed analytics engine that can process large amounts of data with tremendous speed. PySpark is simply the python API for Spark that allows you to use an easy programming language, like python, and leverage the power of Apache Spark.\n",
    "\n",
    "My interest in putting together this example was to learn and prototype. More specifically, learn more about PySpark pipelines as well as how I could integrate deep learning into the PySpark pipeline. I ran this entire project using Jupyter on my local machine to build a prototype for an upcoming data science project where the data will be massive. Since I work for IBM, I'll take this entire analytics project (Jupyter Notebook) and move it to IBM. This allows me to do my data ingestion, pipelining, training and deployment on a unified platform and on a much larger Spark cluster. Obviously, if you had a real and sizable project or using image data you would NOT do this on your local machine.\n",
    "\n",
    "Overall, I found it not too difficult to put together this prototype or working example so I hope others will find it useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 1:</b> Import Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 2:</b> Start Spark Session\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a version of Spark < 2.x your Spark session code may be slightly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "conf = SparkConf().setAppName('Spark DL Tabular Pipeline').setMaster('local[6]')\n",
    "sc = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-DA4TR9N:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark DL Tabular Pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[6] appName=Spark DL Tabular Pipeline>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 3:</b> Load and Preview Data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll load the data. The data we'll use comes from a [Kaggle competition](https://www.kaggle.com/janiobachmann/bank-marketing-dataset). It's a typical banking dataset. I use the `inferSchema` parameter here which helps to identify the feature types when loading in the data. Per the [PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) this _\"requires one extra pass over the data\"_. Since the bank data I'm loading only has ~11k observations it doesnt take long at all, but it may be worth noting if you have a very large dataset.\n",
    "\n",
    "After we load the data we can see the schema and the various feature types. All our features are either `string` type or `integer`. We then preview the first 5 observations. I'm pretty familair with with Pandas python library so through this example you'll see me use `toPandas()` to convert the spark dataframe to a pandas dataframe and do some manipulations. Not right or wrong, just easier for me.\n",
    "\n",
    "Finally, we'll drop the 2 date columns since we won't be using those in our deep learning model. They could be possibly significant and featurized by I decided to just drop them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data to Spark Dataframe\n",
    "df = sql_context.read.csv('bank.csv',\n",
    "                    header=True,\n",
    "                    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age         job  marital  education default  balance housing loan  \\\n",
       "0   1   59      admin.  married  secondary      no     2343     yes   no   \n",
       "1   2   56      admin.  married  secondary      no       45      no   no   \n",
       "2   3   41  technician  married  secondary      no     1270     yes   no   \n",
       "3   4   55    services  married  secondary      no     2476     yes   no   \n",
       "4   5   54      admin.  married   tertiary      no      184      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome deposit  \n",
       "0  unknown    5   may      1042         1     -1         0  unknown     yes  \n",
       "1  unknown    5   may      1467         1     -1         0  unknown     yes  \n",
       "2  unknown    5   may      1389         1     -1         0  unknown     yes  \n",
       "3  unknown    5   may       579         1     -1         0  unknown     yes  \n",
       "4  unknown    5   may       673         2     -1         0  unknown     yes  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview Dataframe (Pandas Preview is Cleaner)\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnessary Features (Day and Month)\n",
    "df = df.drop('day', 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age         job  marital  education default  balance housing loan  \\\n",
       "0   1   59      admin.  married  secondary      no     2343     yes   no   \n",
       "1   2   56      admin.  married  secondary      no       45      no   no   \n",
       "2   3   41  technician  married  secondary      no     1270     yes   no   \n",
       "3   4   55    services  married  secondary      no     2476     yes   no   \n",
       "4   5   54      admin.  married   tertiary      no      184      no   no   \n",
       "\n",
       "   contact  duration  campaign  pdays  previous poutcome deposit  \n",
       "0  unknown      1042         1     -1         0  unknown     yes  \n",
       "1  unknown      1467         1     -1         0  unknown     yes  \n",
       "2  unknown      1389         1     -1         0  unknown     yes  \n",
       "3  unknown       579         1     -1         0  unknown     yes  \n",
       "4  unknown       673         2     -1         0  unknown     yes  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview Dataframe\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 4:</b> Create the Spark Data Pipeline\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the pipeline using PySpark. This essentially takes your data and, per the feature lists you pass, will do the transformations and vectorizing so it is ready for modeling. I referenced the [\"Extracting, transforming and selecting features\"](https://spark.apache.org/docs/latest/ml-features.html) Apache Spark documentation a lot for this pipeline and project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper function to select from the numeric features which ones to standardize based on the kurtosis or skew of that feature. The current defaults for `upper_skew` and `lower_skew` are just general guidelines (depending where you read), but you can modify the upper and lower skew as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to select features to scale given their skew\n",
    "def select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['']):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get into the actual data pipeline. The feature list selection part can be further enhanced to be more dynamic vs listing out each feature, but for this small dataset I just left it as is with `cat_features`, `num_features`, and `label`. Selecting features by type can be done similar to how I did it in the `select_features_to_scale` helper function using something like this `spark_df.toPandas().select_dtypes(include=['object']).columns)` which would return a list of all the columns in your spark dataframe that are object or string type.\n",
    "\n",
    "The first thing we want to do is create an empty list called `stages`. This will contain each step that the data pipeline needs to to complete all transformations within our pipeline. I print out each step of the stages after the pipeline so you can see the sequential steps from my code to a list.\n",
    "\n",
    "The second part is going to be a basic loop to go through each categorical feature from our list `cat_features` and then index and encode those features using one-hot encoding. `StringIndexer` encodes your categorical feature to a feature index with the highest frequency label (count) as feature index `0` and so on. I will preview the transformed data frame after the pipeline, Step 5, where you can see each feature index created from the categorical features. For more information and a basic example of StringIndexer check the [here](https://spark.apache.org/docs/latest/ml-features.html#stringindexer)\n",
    "\n",
    "Within the loop we also do some one-hot encoding (OHE) using the `OneHotEncoderEstimator`. This function only takes a label index so if you have categorical data (objects or strings) you have to use `StringIndexer` so you can pass a label index to the OHE estimator. One nice thing I found from looking at dozens of examples was that you can chain `StringIndexer` output right into the OHE estimator using `string_indexer.getOutputCol()`. If you have a lot of features to transform you'll want to do some thinking about the names, `OutputCol`, because you can't just overwrite feature names so get creative. We'll append all those pipeline steps within our loop into our pipeline list `stages`.\n",
    "\n",
    "Next we use `StringIndexer` again on our label feature or dependent variable. And then we'll move on to scaling the numeric variables using the `select_features_to_scale` helper function from above. Once that list is selected we'll vectorize those features using `VectorAssembler` and then standardize the features within that vector using `StandardScaler`. Then we append those steps to our ongoing pipeline list `stages`.\n",
    "\n",
    "The last step is just assembling all our features into a single vector. We'll find the numeric features from the list `num_features` that were not scaled by just using the difference between our `unscaled_features` (the name of the selected numeric feature TO scale) list and the original list of numeric features `num_features`. Then we assemble or vectorize all the categorical OHE features and numeric features and add that step to our pipeline `stages`. And finally, we add in the `scaled_features` to our `assembled_inputs` to get a final and single vector of features for our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Pipeline\n",
    "cat_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "num_features = ['age','balance','duration','campaign','pdays','previous']\n",
    "label = 'deposit'\n",
    "\n",
    "# Pipeline Stages List\n",
    "stages = []\n",
    "\n",
    "# Loop for StringIndexer and OHE for Categorical Variables\n",
    "for features in cat_features:\n",
    "    \n",
    "    # Index Categorical Features\n",
    "    string_indexer = StringIndexer(inputCol=features, outputCol=features + \"_index\")\n",
    "    \n",
    "    # One Hot Encode Categorical Features\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[string_indexer.getOutputCol()],\n",
    "                                     outputCols=[features + \"_class_vec\"])\n",
    "    # Append Pipeline Stages\n",
    "    stages += [string_indexer, encoder]\n",
    "    \n",
    "# Index Label Feature\n",
    "label_str_index =  StringIndexer(inputCol=label, outputCol=\"label_index\")\n",
    "\n",
    "# Scale Feature: Select the Features to Scale using helper 'select_features_to_scale' function above and Standardize \n",
    "unscaled_features = select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['id'])\n",
    "\n",
    "unscaled_assembler = VectorAssembler(inputCols=unscaled_features, outputCol=\"unscaled_features\")\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "stages += [unscaled_assembler, scaler]\n",
    "\n",
    "# Create list of Numeric Features that Are Not Being Scaled\n",
    "num_unscaled_diff_list = list(set(num_features) - set(unscaled_features))\n",
    "\n",
    "# Assemble or Concat the Categorical Features and Numeric Features\n",
    "assembler_inputs = [feature + \"_class_vec\" for feature in cat_features] + num_unscaled_diff_list\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"assembled_inputs\") \n",
    "\n",
    "stages += [label_str_index, assembler]\n",
    "\n",
    "# Assemble Final Training Data of Scaled, Numeric, and Categorical Engineered Features\n",
    "assembler_final = VectorAssembler(inputCols=[\"scaled_features\",\"assembled_inputs\"], outputCol=\"features\")\n",
    "\n",
    "stages += [assembler_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the steps within our pipeline by looking at our `stages` list that we've been sequentially adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_3719ccf13f3c,\n",
       " OneHotEncoderEstimator_e15712f464a0,\n",
       " StringIndexer_77fc29615f5b,\n",
       " OneHotEncoderEstimator_10278aea9d32,\n",
       " StringIndexer_74baedac3d0b,\n",
       " OneHotEncoderEstimator_998827dcc1fd,\n",
       " StringIndexer_beb387f10dcb,\n",
       " OneHotEncoderEstimator_ec9f0574d0e2,\n",
       " StringIndexer_decff0168c6b,\n",
       " OneHotEncoderEstimator_979b0e951c93,\n",
       " StringIndexer_543023c43bce,\n",
       " OneHotEncoderEstimator_c2261bdc8b82,\n",
       " StringIndexer_b11a31cd1093,\n",
       " OneHotEncoderEstimator_bce6727bb8e4,\n",
       " StringIndexer_54c1022887f8,\n",
       " OneHotEncoderEstimator_cf1448778686,\n",
       " VectorAssembler_eb3ee9f742d9,\n",
       " StandardScaler_63eee29b1699,\n",
       " StringIndexer_2b9ca1da02b0,\n",
       " VectorAssembler_c0ad8b8db00c,\n",
       " VectorAssembler_b04558c9d89a]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 5:</b> Run Data Through the Spark Pipeline\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the _\"hard\"_ part is over we can simply pipeline the stages and fit our data to the pipeline by using `fit()`. Then we actually transform the data by using `transform`. We can now preview our newly transformed PySpark dataframe with all the original and transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit Pipeline to Data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "# Transform Data using Fitted Pipeline\n",
    "df_transform = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_class_vec</th>\n",
       "      <th>contact_index</th>\n",
       "      <th>contact_class_vec</th>\n",
       "      <th>poutcome_index</th>\n",
       "      <th>poutcome_class_vec</th>\n",
       "      <th>unscaled_features</th>\n",
       "      <th>scaled_features</th>\n",
       "      <th>label_index</th>\n",
       "      <th>assembled_inputs</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>[2343.0, 1042.0, 1.0, -1.0, 0.0]</td>\n",
       "      <td>[0.7264185278681131, 3.0017712260834295, 0.367...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.7264185278681131, 3.0017712260834295, 0.367...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>[45.0, 1467.0, 1.0, -1.0, 0.0]</td>\n",
       "      <td>[0.013951700279157103, 4.226102100445672, 0.36...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.013951700279157103, 4.226102100445672, 0.36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>[1270.0, 1389.0, 1.0, -1.0, 0.0]</td>\n",
       "      <td>[0.39374798565621155, 4.001401375268602, 0.367...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.39374798565621155, 4.001401375268602, 0.367...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>[2476.0, 579.0, 1.0, -1.0, 0.0]</td>\n",
       "      <td>[0.7676535531376218, 1.667970767660562, 0.3673...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.7676535531376218, 1.667970767660562, 0.3673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>[184.0, 673.0, 2.0, -1.0, 0.0]</td>\n",
       "      <td>[0.05704695225255348, 1.938763949284211, 0.734...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.05704695225255348, 1.938763949284211, 0.734...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age         job  marital  education default  balance housing loan  \\\n",
       "0   1   59      admin.  married  secondary      no     2343     yes   no   \n",
       "1   2   56      admin.  married  secondary      no       45      no   no   \n",
       "2   3   41  technician  married  secondary      no     1270     yes   no   \n",
       "3   4   55    services  married  secondary      no     2476     yes   no   \n",
       "4   5   54      admin.  married   tertiary      no      184      no   no   \n",
       "\n",
       "   contact  ...  loan_class_vec  contact_index  contact_class_vec  \\\n",
       "0  unknown  ...           (1.0)            1.0         (0.0, 1.0)   \n",
       "1  unknown  ...           (1.0)            1.0         (0.0, 1.0)   \n",
       "2  unknown  ...           (1.0)            1.0         (0.0, 1.0)   \n",
       "3  unknown  ...           (1.0)            1.0         (0.0, 1.0)   \n",
       "4  unknown  ...           (1.0)            1.0         (0.0, 1.0)   \n",
       "\n",
       "   poutcome_index poutcome_class_vec                 unscaled_features  \\\n",
       "0             0.0    (1.0, 0.0, 0.0)  [2343.0, 1042.0, 1.0, -1.0, 0.0]   \n",
       "1             0.0    (1.0, 0.0, 0.0)    [45.0, 1467.0, 1.0, -1.0, 0.0]   \n",
       "2             0.0    (1.0, 0.0, 0.0)  [1270.0, 1389.0, 1.0, -1.0, 0.0]   \n",
       "3             0.0    (1.0, 0.0, 0.0)   [2476.0, 579.0, 1.0, -1.0, 0.0]   \n",
       "4             0.0    (1.0, 0.0, 0.0)    [184.0, 673.0, 2.0, -1.0, 0.0]   \n",
       "\n",
       "                                     scaled_features label_index  \\\n",
       "0  [0.7264185278681131, 3.0017712260834295, 0.367...         1.0   \n",
       "1  [0.013951700279157103, 4.226102100445672, 0.36...         1.0   \n",
       "2  [0.39374798565621155, 4.001401375268602, 0.367...         1.0   \n",
       "3  [0.7676535531376218, 1.667970767660562, 0.3673...         1.0   \n",
       "4  [0.05704695225255348, 1.938763949284211, 0.734...         1.0   \n",
       "\n",
       "                                    assembled_inputs  \\\n",
       "0  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                            features  \n",
       "0  (0.7264185278681131, 3.0017712260834295, 0.367...  \n",
       "1  (0.013951700279157103, 4.226102100445672, 0.36...  \n",
       "2  (0.39374798565621155, 4.001401375268602, 0.367...  \n",
       "3  (0.7676535531376218, 1.667970767660562, 0.3673...  \n",
       "4  (0.05704695225255348, 1.938763949284211, 0.734...  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview Newly Transformed Data\n",
    "df_transform.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Structure Type is a PySpark Dataframe\n",
    "type(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 6:</b> Final Data Prep before Deep Learning Model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple last and quick things we need to do before modeling. First is to create a PySpark dataframe that only contains 2 vectors from the recently transformed dataframe. We only need the: `features` (X) and `label_index` (y) features for modeling. It's easy enough to do with PySpark with the simple `select` statement. Then, just cause, we preview the dataframe.\n",
    "\n",
    "Finally, we want to shuffle our dataframe and then split the data into train and test sets. You always want to shuffle the data prior to modeling to avoid any bias from how the data may be sorted or otherwise organized and specifically shuffling prior to splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.7264185278681131, 3.0017712260834295, 0.367...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.013951700279157103, 4.226102100445672, 0.36...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.39374798565621155, 4.001401375268602, 0.367...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.7676535531376218, 1.667970767660562, 0.3673...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.05704695225255348, 1.938763949284211, 0.734...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label_index\n",
       "0  (0.7264185278681131, 3.0017712260834295, 0.367...          1.0\n",
       "1  (0.013951700279157103, 4.226102100445672, 0.36...          1.0\n",
       "2  (0.39374798565621155, 4.001401375268602, 0.367...          1.0\n",
       "3  (0.7676535531376218, 1.667970767660562, 0.3673...          1.0\n",
       "4  (0.05704695225255348, 1.938763949284211, 0.734...          1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only 'features' and 'label_index' for Final Dataframe\n",
    "df_transform_fin = df_transform.select('features','label_index')\n",
    "df_transform_fin.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Data\n",
    "df_transform_fin = df_transform_fin.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Train / Test Sets\n",
    "train_data, test_data = df_transform_fin.randomSplit([.8, .2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 7:</b> Build a Deep Learning Model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now build a basic deep learning model using Keras. Keras is described as: _\"a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.\"_ in the [Keras documentation](https://keras.io/). I find Keras to be one of the easiest deep learning APIs for python. Also, I found an extension of Keras that allowed me to do easy distributed deep learning on Spark that could integrate with my PySpark pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to determine the number of classes as well as the number of inputs from our data so we can plug those values into our Keras deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Classes\n",
    "nb_classes = train_data.select(\"label_index\").distinct().count()\n",
    "\n",
    "# Number of Inputs or Input Dimensions\n",
    "input_dim = len(train_data.select(\"features\").first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a basic deep learning model. Using the `model = Sequential()` feature from Keras, it's easy to simply add layers and build a deep learning model the all the desired settings (# of units, dropout %, regularization - l2, activation functions, etc.) I selected the common Adam optimizer and binary cross-entropy since out outcome label is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Set up Deep Learning Model / Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(input_dim,), activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(256, activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is built we can view the architecture. Notice that we went from 30 inputs/parameters to 74,242. The beauty (sometimes laziness :) ) of deep learning is the automatic feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 74,242\n",
      "Trainable params: 74,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 8:</b> Distributed Deep Learning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model built, using Keras as our deep learning framework, we want to run that model on Spark to leverage its distributed analytic engine. We do that by using a python library and an extension to Keras called [Elephas](https://github.com/maxpumperla/elephas). Elephas makes it pretty easy to run your Keras models on Apache spark with few lines of configuration. I found Elephas to be easier and more stable to use than the several other libraries I read about and tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do with Elephas is create an estimator similar to some of the PySpark pipeline items above. We can set the optimizer settings right from Keras optimizer function and then pass that to our Elephas estimator. I only explicitly use Adam optimizer with a set learning rate, but you can use any [Keras optimizer](https://keras.io/optimizers/) with their respective parameters (clipnorm, beta_1, beta_2, etc.).\n",
    "\n",
    "Then within the Elephas estimator you specify a variety of items: features column, label column, # of epochs, batch size for training, validation split of your training data, loss function, metric, etc. I just used the settings from an [Elephas example](https://github.com/maxpumperla/elephas/blob/master/examples/ml_pipeline_otto.py) and modified the code slightly.\n",
    "\n",
    "Notice that after we run the estimator the output, `ElephasEstimator_31afcd77fffd`, looks similar to one of our pipeline `stages` list items. This can be passed directly into our PySpark pipeline to fit and transform our data which we'll do in the next step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_35cd35f33ea8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set and Serialize Optimizer\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)\n",
    "\n",
    "# Initialize SparkML Estimator and Get Settings\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"features\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "estimator.set_keras_model_config(model.to_yaml())\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(25) \n",
    "estimator.set_batch_size(64)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"binary_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 9:</b> Distributed Deep Learning Pipeline and Results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that are deep learning model is to be run on Spark, using Elephas, we can pipeline line it exactly how we did above using `Pipeline()`. You could append this to our `stages` list and do all of this with one pass with a new dataset now that it's all built out which would be super cool!\n",
    "\n",
    "I created another helper function below called `dl_pipeline_fit_score_results` that takes the deep learning pipeline `dl_pipeline` and then does all the fitting, transforming, and prediction on both the train and test data sets. It also outputs the accuracy for both data sets and their confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deep Learning Pipeline\n",
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_pipeline_fit_score_results(dl_pipeline=dl_pipeline,\n",
    "                                  train_data=train_data,\n",
    "                                  test_data=test_data,\n",
    "                                  label='label_index'):\n",
    "    \n",
    "    fit_dl_pipeline = dl_pipeline.fit(train_data)\n",
    "    pred_train = fit_dl_pipeline.transform(train_data)\n",
    "    pred_test = fit_dl_pipeline.transform(test_data)\n",
    "    \n",
    "    pnl_train = pred_train.select(label, \"prediction\")\n",
    "    pnl_test = pred_test.select(label, \"prediction\")\n",
    "    \n",
    "    pred_and_label_train = pnl_train.rdd.map(lambda row: (row[label], row['prediction']))\n",
    "    pred_and_label_test = pnl_test.rdd.map(lambda row: (row[label], row['prediction']))\n",
    "    \n",
    "    metrics_train = MulticlassMetrics(pred_and_label_train)\n",
    "    metrics_test = MulticlassMetrics(pred_and_label_test)\n",
    "    \n",
    "    print(\"Training Data Accuracy: {}\".format(round(metrics_train.precision(),4)))\n",
    "    print(\"Training Data Confusion Matrix\")\n",
    "    display(pnl_train.crosstab('label_index', 'prediction').toPandas())\n",
    "    \n",
    "    print(\"\\nTest Data Accuracy: {}\".format(round(metrics_test.precision(),4)))\n",
    "    print(\"Test Data Confusion Matrix\")\n",
    "    display(pnl_test.crosstab('label_index', 'prediction').toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new deep learning pipeline and helper function on both data sets and test our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o61.sql.\n: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:183)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\t... 64 more\r\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\t... 79 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\t... 85 more\r\nCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:183)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\t... 147 more\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\burak\\Desktop\\Burak\\itu\\3. Semester\\Machine Learning\\Project\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\t... 144 more\r\n------\r\n\nNestedThrowables:\njava.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:183)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\t... 147 more\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\burak\\Desktop\\Burak\\itu\\3. Semester\\Machine Learning\\Project\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\t... 144 more\r\n------\r\n\r\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\t... 90 more\r\nCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r\njava.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:183)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:117)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\t... 147 more\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\burak\\Desktop\\Burak\\itu\\3. Semester\\Machine Learning\\Project\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\t... 144 more\r\n------\r\n\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)\r\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r\n\t... 119 more\r\nCaused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat java.sql.DriverManager.getConnection(Unknown Source)\r\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r\n\t... 131 more\r\nCaused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@a48e00, see the next exception for details.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\r\n\t... 147 more\r\nCaused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Users\\burak\\Desktop\\Burak\\itu\\3. Semester\\Machine Learning\\Project\\metastore_db.\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\r\n\t... 144 more\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-cb5b6595cf8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                               \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                               \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                               label='label_index');\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-1c2301ef586e>\u001b[0m in \u001b[0;36mdl_pipeline_fit_score_results\u001b[1;34m(dl_pipeline, train_data, test_data, label)\u001b[0m\n\u001b[0;32m      4\u001b[0m                                   label='label_index'):\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mfit_dl_pipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mpred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_dl_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_dl_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\elephas\\ml_model.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \"\"\"\n\u001b[0;32m     75\u001b[0m         simple_rdd = df_to_simple_rdd(df, categorical=self.get_categorical_labels(), nb_classes=self.get_nb_classes(),\n\u001b[1;32m---> 76\u001b[1;33m                                       features_col=self.getFeaturesCol(), label_col=self.getLabelCol())\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0msimple_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_workers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mkeras_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_yaml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_keras_model_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\elephas\\ml\\adapter.py\u001b[0m in \u001b[0;36mdf_to_simple_rdd\u001b[1;34m(df, categorical, nb_classes, features_col, label_col)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0msql_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregisterDataFrameAsTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"temp_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     selected_df = sql_context.sql(\n\u001b[1;32m---> 32\u001b[1;33m         \"SELECT {0} AS features, {1} as label from temp_table\".format(features_col, label_col))\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLLibVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         lp_rdd = selected_df.rdd.map(\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\context.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \"\"\"\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \"\"\"\n\u001b[1;32m--> 767\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.4.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'"
     ]
    }
   ],
   "source": [
    "dl_pipeline_fit_score_results(dl_pipeline=dl_pipeline,\n",
    "                              train_data=train_data,\n",
    "                              test_data=test_data,\n",
    "                              label='label_index');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope that this example has been helpful. I know it was for me in learning more about PySpark pipelines and doing deep learning on spark using an easy deep learning framework like Keras. Like I mentioned, I ran all this locally with little to no issue. My main objective was to prototype something for an upcoming project that will contain a massive dataset. Luckily working for IBM allows me to leverage Watson so I'll train and deploy on a large Spark cluster. My hope is that you (and myself) can use this as a template while making few changes to things like the spark session, data, feature selection, and maybe adding or removing some pipeline stages. As always, thanks for reading and good luck on your next project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,663,370\n",
      "Trainable params: 1,663,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The data format channels_last is not supported. Please use keras.backend.set_image_data_format(\"channels_first\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9d45d48aee4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m# Now create a SystemML model by calling the Keras2DML method and feeding it your spark session, Keras model, its input shape, and the  # predefined variables. We also ask to be displayed the traning results every 10 iterations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0msysml_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeras2DML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weights_dir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# Initiate traning. More spark workers and better machine configuration means faster training!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\systemml\\mllearn\\estimators.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sparkSession, keras_model, input_shape, transferUsingDF, load_keras_weights, weights, labels, batch_size, max_iter, test_iter, test_interval, display, lr_policy, weight_decay, regularization_type)\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m             raise Exception('The data format ' + str(keras.backend.image_data_format())\n\u001b[1;32m-> 1039\u001b[1;33m                             + ' is not supported. Please use keras.backend.set_image_data_format(\"channels_first\")')\n\u001b[0m\u001b[0;32m   1040\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# Convert the sequential model to functional model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: The data format channels_last is not supported. Please use keras.backend.set_image_data_format(\"channels_first\")"
     ]
    }
   ],
   "source": [
    "################################### Keras2DML: Parallely training neural network with SystemML####################################### \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv1D, Conv2D, MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# Expect to see a numpy n-dimentional array of (60000, 28, 28)\n",
    "\n",
    "type(X_train), X_train.shape, type(X_train)\n",
    "\n",
    "\n",
    "#This time however, we flatten each of our 28 X 28 images to a vector of 1, 784\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# expect to see a numpy n-dimentional array of : (60000, 784) for Traning Data shape and (10000, 784) for Test Data shape\n",
    "type(X_train), X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "#We also use sklearn's MinMaxScaler for normalizing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scaleData(data):\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "X_train = scaleData(X_train)\n",
    "X_test = scaleData(X_test)\n",
    "\n",
    "\n",
    "# We define the same Keras model as earlier\n",
    "\n",
    "input_shape = (1,28,28) if K.image_data_format() == 'channels_first' else (28,28, 1)\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape, padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Flatten())\n",
    "keras_model.add(Dense(512, activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(10, activation='softmax'))\n",
    "keras_model.summary()\n",
    "\n",
    "\n",
    "# Import the Keras to DML wrapper and define some basic variables\n",
    "\n",
    "from systemml.mllearn import Keras2DML\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "samples = 60000\n",
    "max_iter = int(epochs*samples/batch_size)\n",
    "\n",
    "# Now create a SystemML model by calling the Keras2DML method and feeding it your spark session, Keras model, its input shape, and the  # predefined variables. We also ask to be displayed the traning results every 10 iterations.\n",
    "\n",
    "sysml_model = Keras2DML(spark, keras_model, input_shape=(1,28,28), weights='weights_dir', batch_size=batch_size, max_iter=max_iter, test_interval=0, display=10)\n",
    "\n",
    "# Initiate traning. More spark workers and better machine configuration means faster training!\n",
    "\n",
    "sysml_model.fit(X_train, y_train)\n",
    "\n",
    "# Test your model's performance on the secluded test set, and re-iterate if required \n",
    "sysml_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from itertools import imap\n",
    "except ImportError:\n",
    "    # Python 3...\n",
    "    imap=map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemml import MLContext\n",
    "ml = MLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLContext"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Caffe2DML',\n",
       " 'Keras2DML',\n",
       " 'LinearRegression',\n",
       " 'LogisticRegression',\n",
       " 'NaiveBayes',\n",
       " 'SVM',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'estimators']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import systemml\n",
    "dir(systemml.mllearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session    \n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=tf_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,663,370\n",
      "Trainable params: 1,663,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 0.1974 - acc: 0.9388\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.0603 - acc: 0.9816\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 125s 2ms/step - loss: 0.0422 - acc: 0.9870\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 125s 2ms/step - loss: 0.0319 - acc: 0.9900\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.0266 - acc: 0.9919\n",
      "training time: 625.7936305999756\n",
      "10000/10000 [==============================] - 9s 920us/step\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18403bf9ef0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################### Keras Only: Training the same Neural network without SystemML####################################### \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv1D, Conv2D, MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Check type and shape of your data\n",
    "type(X_train[0]), X_train.shape, y_train.shape, X_test.shape, y_test.shape   \n",
    "\n",
    "#Plot out the image to see the numbers\n",
    "plt.imshow(X_train[0])#, cmap = plt.cm.binary) \n",
    "\n",
    "\n",
    "# Reshape data for Conv2D layer in our neural network, check keras.io/layers for details \n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "\n",
    "# Check input shape again to confirm. Ideally, you should see: (numpy.ndarray, (60000, 28, 28, 1), (10000, 28, 28, 1))\n",
    "type(X_train[0]), X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "\n",
    " # scaling the pixel values that are usually between 0-255 to a value between 0 and 1.\n",
    " #This makes it easier for the network to learn, experiment without normalization, and youll see the difference in accuracy.\n",
    " \n",
    "x_train = tf.keras.utils.normalize(X_train, axis=1) \n",
    "x_test = tf.keras.utils.normalize(X_test, axis=1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Design your neural network using a Keras Model, and pay attention to the input shape of your data. In our case, we are feeding our network 28X28 pixel vectors\n",
    "\n",
    "input_shape = (1,28,28) if K.image_data_format() == 'channels_first' else (28,28, 1)\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape, padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Flatten())\n",
    "keras_model.add(Dense(512, activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(10, activation='softmax'))\n",
    "keras_model.summary()\n",
    "\n",
    "\n",
    "# Compile your model, choose an appropriate optimizer, loss function and metric to track.\n",
    "\n",
    "keras_model.compile(optimizer='adam',#'sgd'\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy']) \n",
    "             \n",
    "             \n",
    "# Train your model, adjust batch size and epochs iteratively. Optionally time your training.\n",
    " \n",
    "import time\n",
    "start = time.time()\n",
    "keras_model.fit(x_train, y_train, epochs=5, batch_size=100) \n",
    "end=time.time()\n",
    "print(\"training time:\", (end-start))\n",
    "\n",
    "# Test your model on the secluded test set\n",
    "\n",
    "keras_model.evaluate(x_test, y_test)\n",
    "\n",
    "# Make predictions and Reshape your npndarrays to be able to verify your predictions by plotting out the image\n",
    "\n",
    "predictions = keras_model.predict([x_test])\n",
    "\n",
    "x_test = np.squeeze(x_test)\n",
    "x_test.shape\n",
    "\n",
    "# Print the prediction value with the maximum probability assigned by the network for the first image.\n",
    "print(np.argmax(predictions[0]))\n",
    "\n",
    "# Plot out the first image and check if the network got it right\n",
    "plt.imshow(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x184051e62e8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the prediction value with the maximum probability assigned by the network for the first image.\n",
    "print(np.argmax(predictions[1]))\n",
    "\n",
    "# Plot out the first image and check if the network got it right\n",
    "plt.imshow(x_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\burak\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 28, 28)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 14, 14)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 7, 7)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,663,370\n",
      "Trainable params: 1,663,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imap' from 'itertools' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76ac13650443>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# Train Lenet using SystemML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msystemml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeras2DML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0msysml_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeras2DML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weights_dir'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;31m# sysml_model.setConfigProperty(\"sysml.native.blas\", \"auto\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m# sysml_model.setGPU(True).setForceGPU(True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\systemml\\mllearn\\estimators.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sparkSession, keras_model, input_shape, transferUsingDF, load_keras_weights, weights, labels, batch_size, max_iter, test_iter, test_interval, display, lr_policy, weight_decay, regularization_type)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[0mregularization_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mregularization\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"L2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \"\"\"\n\u001b[1;32m-> 1035\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkeras2caffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvertKerasToCaffeNetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvertKerasToCaffeSolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvertKerasToSystemMLModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\systemml\\mllearn\\keras2caffe.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'imap' from 'itertools' (unknown location)"
     ]
    }
   ],
   "source": [
    "# pyspark --driver-memory 20g\n",
    "\n",
    "# Disable Tensorflow from using GPU to avoid unnecessary evictions by SystemML runtime\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import keras\n",
    "# Import dependencies\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "#from mlxtend.data import mnist_data\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Set channel first layer\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# Download the MNIST dataset\n",
    "X, y = mnist.load_data()\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "# Split the data into training and test\n",
    "n_samples = len(X)\n",
    "X_train = X[:int(.9 * n_samples)]\n",
    "y_train = y[:int(.9 * n_samples)]\n",
    "X_test = X[int(.9 * n_samples):]\n",
    "y_test = y[int(.9 * n_samples):]\n",
    "\n",
    "# Define Lenet in Keras\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(1,28,28), padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(Flatten())\n",
    "keras_model.add(Dense(512, activation='relu'))\n",
    "keras_model.add(Dropout(0.5))\n",
    "keras_model.add(Dense(10, activation='softmax'))\n",
    "keras_model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True))\n",
    "keras_model.summary()\n",
    "\n",
    "# Scale the input features\n",
    "#scale = 0.00390625\n",
    "#X_train = X_train*scale\n",
    "#X_test = X_test*scale\n",
    "\n",
    "# Train Lenet using SystemML\n",
    "from systemml.mllearn import Keras2DML\n",
    "sysml_model = Keras2DML(spark, keras_model, weights='weights_dir')\n",
    "# sysml_model.setConfigProperty(\"sysml.native.blas\", \"auto\")\n",
    "# sysml_model.setGPU(True).setForceGPU(True)\n",
    "sysml_model.fit(X_train, y_train)\n",
    "sysml_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATASET_MEAN',\n",
       " 'SUPPORTED_TYPES',\n",
       " 'SparkContext',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_convertDenseMatrixToMB',\n",
       " '_convertSPMatrixToMB',\n",
       " '_copyRowBlock',\n",
       " 'convertImageToNumPyArr',\n",
       " 'convertToLabeledDF',\n",
       " 'convertToMatrixBlock',\n",
       " 'convertToNumPyArr',\n",
       " 'convertToPandasDF',\n",
       " 'convert_caffemodel',\n",
       " 'convert_lmdb_to_jpeg',\n",
       " 'coo_matrix',\n",
       " 'createJavaObject',\n",
       " 'csr_matrix',\n",
       " 'default_jvm_stdout',\n",
       " 'default_jvm_stdout_parallel_flush',\n",
       " 'getDatasetMean',\n",
       " 'getNumCols',\n",
       " 'get_pretty_str',\n",
       " 'get_spark_context',\n",
       " 'jvm_stdout',\n",
       " 'math',\n",
       " 'np',\n",
       " 'os',\n",
       " 'pd',\n",
       " 'save_tensor_csv',\n",
       " 'set_default_jvm_stdout',\n",
       " 'spmatrix']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(systemml.converters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib' has no attribute 'urlretrieve'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1998eed14b0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Download the Lenet network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/mnist_lenet/lenet.proto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lenet.proto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/mnist_lenet/lenet_solver.proto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lenet_solver.proto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Train Lenet On MNIST using scikit-learn like API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'urlretrieve'"
     ]
    }
   ],
   "source": [
    "from systemml.mllearn import Caffe2DML\n",
    "import urllib\n",
    "\n",
    "# Download the Lenet network\n",
    "urllib.urlretrieve('https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/mnist_lenet/lenet.proto', 'lenet.proto')\n",
    "urllib.urlretrieve('https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/mnist_lenet/lenet_solver.proto', 'lenet_solver.proto')\n",
    "# Train Lenet On MNIST using scikit-learn like API\n",
    "\n",
    "# MNIST dataset contains 28 X 28 gray-scale (number of channel=1).\n",
    "lenet = Caffe2DML(spark, solver='lenet_solver.proto', input_shape=(1, 28, 28))\n",
    "lenet.setStatistics(True)\n",
    "# lenet.setConfigProperty(\"sysml.native.blas\", \"auto\")\n",
    "# lenet.setGPU(True).setForceGPU(True)\n",
    "\n",
    "# Since Caffe2DML is a mllearn API, it allows for scikit-learn like method for training.\n",
    "lenet.fit(X_train, y_train)\n",
    "# Either perform prediction: lenet.predict(X_test) or scoring:\n",
    "lenet.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemml import MLContext\n",
    "ml = MLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.keras2caffe'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-84e25a3e4667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkeras2caffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '__main__.keras2caffe'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "from .keras2caffe import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-17 05:58:31 UTC\n"
     ]
    }
   ],
   "source": [
    "print (ml.buildTime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, neighbors\n",
    "from systemml.mllearn import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
